<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: MachineLearning | YY Every Day]]></title>
  <link href="http://arekyao.github.io/tags/machinelearning/atom.xml" rel="self"/>
  <link href="http://arekyao.github.io/"/>
  <updated>2013-10-22T12:10:15+08:00</updated>
  <id>http://arekyao.github.io/</id>
  <author>
    <name><![CDATA[Arek.Yao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine learning ep2]]></title>
    <link href="http://arekyao.github.io/blog/2013/03/09/machine-learning-ep2/"/>
    <updated>2013-03-09T22:56:00+08:00</updated>
    <id>http://arekyao.github.io/blog/2013/03/09/machine-learning-ep2</id>
    <content type="html"><![CDATA[<ul>
  <li>
    <h4 id="discriminative-learning-algorithm">Discriminative learning algorithm</h4>
  </li>
</ul>

<p>learn 
$$
P(y|x)
$$
or</p>

<script type="math/tex; mode=display"> 
h_{\theta}(x) \in \left \{ 0,1 \right \}
</script>

<p>directly</p>

<!-- more -->

<ul>
  <li>
    <h4 id="generative-learning-algorithm">Generative learning algorithm</h4>
  </li>
</ul>

<p>learn
P(x|y)</p>

<p>then
P(y)</p>

<p>such as Native Bayes</p>

<ul>
  <li>
    <h4 id="laplace-smoothing">Laplace smoothing</h4>
  </li>
</ul>

<p>A/B</p>

<p>==&gt;</p>

<p>(A+1)/(B+1)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning ep1]]></title>
    <link href="http://arekyao.github.io/blog/2013/02/16/machine-learning-ep1/"/>
    <updated>2013-02-16T22:35:00+08:00</updated>
    <id>http://arekyao.github.io/blog/2013/02/16/machine-learning-ep1</id>
    <content type="html"><![CDATA[<ul>
  <li>
    <h2 id="the-general-steps-of-machine-learning">The general steps of Machine Learning</h2>
  </li>
</ul>

<h4 id="traning-data--test-data">1, Traning Data &amp; Test Data</h4>

<p>This step is the most important part in machine learning. 
Very easy to ignore and underestimate.</p>

<p>Data is wrong, all the steps afterward are wrong.</p>

<p>More than that, it’s very difficult to understand where is wrong, how to impove that.</p>

<p>Maybe the data can be produced by online logs by user.</p>

<p>Maybe the data can not be gotten from machine, only by humans.</p>

<h4 id="feature-selectionhttpenwikipediaorgwikifeatureselection">2, <a href="http://en.wikipedia.org/wiki/Feature_selection">Feature Selection</a></h4>

<p>The less feature, the faster the model is calculated and updated</p>

<p>The more feature’s meaning is ,the less prabability overfit happens </p>

<p>L1-norm</p>

<p>http://www.cnblogs.com/heaad/archive/2011/01/02/1924088.html</p>

<h4 id="model-choosing">3, Model Choosing</h4>

<p>Different model has its personality</p>

<h4 id="evaluation">4, Evaluation</h4>

<p>RSE</p>

<p>FRTP…</p>

<hr />

<ul>
  <li>
    <h2 id="linear-regressionhttpenwikipediaorgwikilinearregression"><a href="http://en.wikipedia.org/wiki/Linear_regression">Linear Regression</a></h2>
  </li>
</ul>

<p>It’s a simple model,and very useful</p>

<ul>
  <li>
    <h2 id="locally-weighted-linear-regression">Locally Weighted Linear Regression</h2>
  </li>
</ul>

<p>the training data only contain the data near X  (is not it cool?)</p>

<hr />

<ul>
  <li>
    <h2 id="batch-gradient-descent-vs-stochastic-gradient-descent">Batch Gradient Descent vs stochastic gradient descent（随机梯度下降）</h2>
  </li>
</ul>

<h5 id="batch-gradient-descent">Batch Gradient Descent:</h5>

<blockquote>
  <p>You need to run over every training example before doing an update, which means that if you have a large dataset, you might spend much time on getting something that works.</p>
</blockquote>

<h5 id="stochastic-gradient-descent">Stochastic gradient descent:</h5>
<blockquote>
  <p>on the other hand, does updates every time it finds a training example, however, since it only uses one update, it may never converge, although you can still be pretty close to the minimum.</p>
</blockquote>

<p>In conclusion, if your data is large, you might want to use Stochastic Gradient Descent.</p>

<h4 id="newton-method">Newton Method:</h4>
<p>Advantage: fast iteration to get result
Disadvantage: have to invert the N*N’s (N is the number of feature) every step</p>

<ul>
  <li>
    <h2 id="when-to-stop-the-iteration-of-gradient-descent">When to stop the iteration of Gradient Descent</h2>
    <p>It has two ways.</p>
  </li>
</ul>

<p><code>Condition 1: the two iteration's result is so close to ignore</code></p>

<p><code>Condition 2: the loss is very small, which can be ignored</code></p>

<ul>
  <li>parametric learning algorithm &amp; non-parametric learning algorithm</li>
</ul>

<blockquote>
  <p>non-parametric learning: the number of arguments will go with size of training data</p>
</blockquote>

<ul>
  <li>
    <h2 id="exponential-distribution-family">Exponential distribution family</h2>
    <p>eg: Normal Distribution, Poisson Distribution, Bernouli Distribution </p>
  </li>
  <li>
    <h2 id="logistic-regression--softmax-regression">Logistic Regression &amp; Softmax Regression</h2>
    <p>Logistic Regression only 2 kinds
Softmax Regession can solve multi</p>
  </li>
</ul>

<hr />

<h2 id="reference">Reference:</h2>

<p><a href="">http://zhfuzh.blog.163.com/blog/static/14553938720127309539465/</a></p>

<p><a href="">http://blog.csdn.net/pennyliang/article/details/6998517</a></p>

<p><a href="">http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B</a></p>

<p>It’s not finished…</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Overview of Machine Learning]]></title>
    <link href="http://arekyao.github.io/blog/2013/02/15/the-overview-of-machine-learning/"/>
    <updated>2013-02-15T21:19:00+08:00</updated>
    <id>http://arekyao.github.io/blog/2013/02/15/the-overview-of-machine-learning</id>
    <content type="html"><![CDATA[<blockquote>
  <p>First of all, I have to say, the Machine learning series have a very deep water,:)
I dont have enough time to afford a full explaination, in detail, about the machine learning.
It’s just my study map, or so-called logs..</p>
</blockquote>

<p>Here is the stories of it.</p>

<h2 id="machine-learning">Machine Learning</h2>

<!-- more -->

<p><a href="http://en.wikipedia.org/wiki/Machine_learning">Machine learning</a>, a branch of artificial intelligence, is about the construction and study of systems that can learn from data</p>

<p>Refer to the difference between Machine Learning and Data Mining, the DM is the target, the ML is the way.</p>

<blockquote>
  <p>Machine learning focuses on prediction, based on known properties learned from the training data.
Data mining (which is the analysis step of Knowledge Discovery in Databases) focuses on the discovery of (previously) unknown properties on the data.</p>
</blockquote>

<p>Algorithms Type OF ML</p>

<ul>
  <li>
    <h2 id="supervised-learning">1,Supervised learning</h2>
  </li>
</ul>

<blockquote>
  <p>generates a function that maps inputs to desired outputs (also called labels, because they are often provided by human experts labeling the training examples). For example, in a <code>classification</code> problem, the learner approximates a function mapping a vector into classes by looking at input-output examples of the function.</p>
</blockquote>

<h3 id="decision-tree">+ Decision Tree</h3>

<h3 id="association-rule-learning">+ Association rule learning</h3>

<h3 id="svm">+ SVM</h3>

<h3 id="random-forrest">+ Random Forrest</h3>

<h3 id="naive-bayes-classifier">+ Naive bayes classifier</h3>

<h3 id="linearregression">+ Linear_regression</h3>

<h3 id="section">…</h3>

<ul>
  <li>
    <h2 id="unsupervised-learning">2,Unsupervised learning</h2>
    <blockquote>
      <p>models a set of inputs, like <code>clustering</code>. See also data mining and knowledge discovery. Here, labels are not known during training.</p>
    </blockquote>
  </li>
  <li>
    <h2 id="semi-supervised-learning">3,Semi-supervised learning</h2>
  </li>
</ul>

<blockquote>
  <p>combines both labeled and unlabeled examples to generate an appropriate function or classifier. Transduction, or transductive inference, tries to predict new outputs on specific and fixed (test) cases from observed, specific (training) cases.</p>
</blockquote>

<ul>
  <li>
    <h2 id="reinforcement-learning">4,Reinforcement learning</h2>
    <blockquote>
      <p>learns how to act given an observation of the world. Every action has some impact in the environment, and the environment provides feedback in the form of rewards that guides the learning algorithm.</p>
    </blockquote>
  </li>
  <li>
    <h2 id="learning-to-learn">5,Learning to learn</h2>
    <p>learns its own inductive bias based on previous experience.</p>
  </li>
</ul>

<h2 id="reference">Reference:</h2>
<p>1, <a href="http://v.163.com/special/opencourse/machinelearning.html">网易公开课</a> </p>

<p>2, <a href="http://zhfuzh.blog.163.com/blog/static/14553938720127309539465">行走的馒头</a> </p>

<p>3, <a href="http://cs229.stanford.edu/materials.html">Stanford’s lesson</a></p>

<p>4, <a href="http://www.datasciencecentral.com/profiles/blogs/berkeley-course-on-data-science">Berkeley course on Data Science,北美+德国18名校的数据挖掘、数据分析、人工智能及机器学习课程资源汇总</a></p>
]]></content>
  </entry>
  
</feed>
